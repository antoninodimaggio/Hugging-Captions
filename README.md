# Hugging-Captions

![example](https://drive.google.com/uc?id=1kKYG7n2O7kv1Kg1Ef72Hzzdz8WkU_G79)

<i>Photo is from Rocky Mountain National Park</i>

Captions for the above photo generated by Hugging Captions (the last one is amazing). Trained with #rmnp captions. Captions generated using the prompt "The view".
* The view from the top of the mountain - one of our favorite hikes in Rocky Mountain National Park.
* The view is stunning in all its glory! ðŸ’•ðŸƒ
* The view of the chasm opens up once more when the vast expanses of the Rocky Mountains re-emerge into the background like the morning sun behind the tree line! #rmnp

## Introduction

Hugging Captions fine-tunes [GPT-2](https://openai.com/blog/better-language-models/), a transformer-based language model by [OpenAI](https://openai.com/), to generate realistic captions. The model is trained using cleaned caption data obtained from realevant instagram hashtags. The fine-tuned model generates captions given a starting prompt of about 2-5 words. All of the transformer stuff is implemented using [Hugging Face's Transformers library](https://github.com/huggingface/transformers).

## Requirements
* A CUDA enabled device ~ (I used version 10.2)
* Python 3.6 ~ (not sure about compatibility with other Python 3 versions)
```
git clone https://github.com/antoninodimaggio/Hugging-Captions.git
pip install -r requirements.txt
pip install git+https://github.com/antoninodimaggio/instagram-scraper.git@hugging_captions
```

## Download and Clean The Data
* Make sure that the hashtag you choose has a decent amount of posts (>=5000) and is relevant to the photo you want to generate a caption for. This is very important.
* Detailed information on each argument can be found [here](DOCS.md)
* You could also use python pull_and_clean.py -h for help
```
python pull_and_clean.py --tag <some hashtag> \
    --caption-queries <number of queries> \
    --min-likes <min number of likes>
```

## Fine-tune The Model
* Now that we have our training data we can fine-tune our transformer-based language model.
* I trained my model with ~3000 lines of text on one GeFore RTX 2080 TI with 11019MiB of available memory. This took less than 5 minutes. GPU memory can be an issue.
* Detailed information on each argument can be found [here](DOCS.md)
* You could also use python tune_transformer.py -h for help <br><br>
Just fine-tune the model which is saved at ./trained_models/somehashtag/
```
python tune_transformer.py --tag <some hashtag> --train
```
Just generate captions which are written to ./text/generated_text/somehashtag_gen.txt
```
python tune_transformer.py --tag <some hashtag> --generate \
    --max-length <max length of caption> \
    --min-length <min length of caption>
```
Fine-tune and generate at the same time
```
python tune_transformer.py --tag <some hashtag> --train --generate \
    --max-length <max length of caption> \
    --min-length <min length of caption>
```

## Future Work
* Explore ways to better clean caption data both generated and training
* Explore different pre-trained language models
* Fine-tune models using caption data from mutiple realevant hashtags

## Intended Use Of This Project
<b>This project was created, and is intended to be used, for research purposes only. This project is not to be used to publish generated text for puroses other than those previously stated.</b>
